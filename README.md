# MLOps Pipeline - PredicciÃ³n de DeserciÃ³n Estudiantil

![Python](https://img.shields.io/badge/python-v3.9+-blue.svg)
![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-2.9.2-red.svg)
![Docker](https://img.shields.io/badge/docker-%230db7ed.svg)

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un pipeline MLOps completo para la predicciÃ³n de deserciÃ³n estudiantil utilizando Apache Airflow como orquestador. El sistema automatiza el procesamiento de datos, entrenamiento de modelos y generaciÃ³n de reportes mediante contenedores Docker.

## ğŸ—ï¸ Arquitectura

```
â”œâ”€â”€ dags/                    # DAGs de Airflow
â”‚   â””â”€â”€ mlops_dag.py        # Pipeline principal de ML
â”œâ”€â”€ src/                    # CÃ³digo fuente
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ processing.py   # Funciones de procesamiento
â”œâ”€â”€ volumes/                # VolÃºmenes persistentes
â”‚   â””â”€â”€ airflow/           # ConfiguraciÃ³n de Airflow
â”œâ”€â”€ docker-compose.yml      # OrquestaciÃ³n de servicios
â”œâ”€â”€ Dockerfile             # Imagen personalizada
â””â”€â”€ requirements.txt       # Dependencias Python
```

## ğŸš€ CaracterÃ­sticas

- **Pipeline Automatizado**: Procesamiento completo de datos desde ingesta hasta predicciÃ³n
- **Procesamiento Paralelo**: Manejo simultÃ¡neo de duplicados y outliers
- **ContainerizaciÃ³n**: Despliegue consistente con Docker
- **Monitoreo**: Interface web de Airflow para seguimiento
- **Escalabilidad**: Arquitectura modular y extensible

## ğŸ› ï¸ TecnologÃ­as

- **Apache Airflow 2.9.2**: OrquestaciÃ³n de workflows
- **Python 3.9+**: Lenguaje principal
- **Docker & Docker Compose**: ContainerizaciÃ³n
- **pandas**: ManipulaciÃ³n de datos
- **scikit-learn**: Machine Learning
- **MySQL 8.0**: Base de datos (opcional)

## âš™ï¸ InstalaciÃ³n

### Prerrequisitos

- Docker >= 20.10
- Docker Compose >= 2.0
- Git

### ConfiguraciÃ³n del Entorno

1. **Clonar el repositorio**:
```bash
git clone <repository-url>
cd mlops-pipeline
```

2. **Preparar la estructura de datos**:
```bash
# Crear directorio de datos (debe contener dataset.csv)
mkdir -p ../egi-control/data/raw
# Copiar tu dataset a ../egi-control/data/raw/dataset.csv
```

3. **Levantar los servicios**:
```bash
docker-compose up -d
```

4. **Verificar la instalaciÃ³n**:
- Acceder a Airflow UI: http://localhost:8080
- Credenciales por defecto: `admin/admin`

## ğŸ¯ Uso

### Ejecutar el Pipeline

1. **Acceder a Airflow UI**: http://localhost:8080
2. **Activar el DAG**: `mlops_dropout_prediction`
3. **Trigger manual** o esperar ejecuciÃ³n programada (diaria)

### Estructura del Pipeline

El DAG `mlops_dropout_prediction` ejecuta las siguientes tareas:

```mermaid
graph TD
    A[Start] --> B[Load Data]
    B --> C[Handle Missing Values]
    C --> D[Remove Duplicates]
    C --> E[Filter Outliers]
    D --> F[Create Features]
    E --> F
    F --> G[Generate Report]
    G --> H[End]
```

#### Tareas del Pipeline:

1. **load_data**: Carga el dataset desde `/opt/airflow/data/raw/dataset.csv`
2. **handle_missing_values**: Imputa valores faltantes
3. **remove_duplicates**: Elimina filas duplicadas
4. **filter_outliers**: Filtra outliers usando Z-score (threshold=3)
5. **create_features**: Crea caracterÃ­sticas derivadas (`total_score`)
6. **generate_report**: Genera reporte de limpieza

### Archivos Generados

```
/opt/airflow/data/
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ dataset.csv              # Dataset original
â”œâ”€â”€ temp/                        # Archivos intermedios
â”‚   â”œâ”€â”€ dataset_temp.csv
â”‚   â”œâ”€â”€ step1_missing_handled.csv
â”‚   â”œâ”€â”€ step2_duplicates_removed.csv
â”‚   â””â”€â”€ step2b_outliers_filtered.csv
â””â”€â”€ processed/                   # Resultados finales
    â”œâ”€â”€ cleaned_data.csv         # Dataset limpio
    â””â”€â”€ cleaning_report.txt      # Reporte de mÃ©tricas
```

## ğŸ“Š ConfiguraciÃ³n

### Variables de Entorno

Modificar en `docker-compose.yml`:

```yaml
environment:
  - AIRFLOW__CORE__EXECUTOR=SequentialExecutor  # Para producciÃ³n usar CeleryExecutor
  - AIRFLOW__CORE__LOAD_EXAMPLES=false
  - AIRFLOW__CORE__FERNET__KEY=<your-key>
```

### PersonalizaciÃ³n del Pipeline

#### Modificar rutas de datos:
```python
# En dags/mlops_dag.py
RAW_DATA_PATH = "/ruta/a/tu/dataset.csv"
TEMP_DIR = "/ruta/temporal"
PROCESSED_DIR = "/ruta/procesados"
```

#### Ajustar parÃ¡metros de limpieza:
```python
# Threshold para outliers
def filter_numeric_outliers(df, threshold=3):  # Cambiar threshold
```

#### ProgramaciÃ³n del DAG:
```python
# En la definiciÃ³n del DAG
schedule_interval="@daily"  # Cambiar frecuencia
```

## ğŸ”§ Desarrollo

### Agregar nuevas tareas

1. **Crear funciÃ³n de procesamiento**:
```python
def nueva_tarea():
    # Tu lÃ³gica aquÃ­
    pass
```

2. **Agregar PythonOperator**:
```python
nueva_task = PythonOperator(
    task_id="nueva_tarea",
    python_callable=nueva_tarea
)
```

3. **Definir dependencias**:
```python
tarea_anterior >> nueva_task >> tarea_siguiente
```

### Funciones Utilitarias

Agregar funciones reutilizables en `src/utils/processing.py`:

```python
def mi_funcion(**kwargs):
    input_path = kwargs['input_path']
    output_path = kwargs['output_path']
    # Procesamiento
```

## ğŸ› Troubleshooting

### Problemas Comunes

**Error: "No such file or directory"**
```bash
# Verificar que el dataset existe
ls -la ../egi-control/data/raw/dataset.csv
```

**Airflow UI no carga**
```bash
# Reiniciar servicios
docker-compose restart airflow
```

**Error de permisos**
```bash
# Ajustar permisos de volÃºmenes
sudo chown -R $USER:$USER volumes/
```

**Ver logs detallados**
```bash
# Logs de Airflow
docker-compose logs airflow

# Logs del scheduler
docker-compose logs airflow-scheduler
```

### Comandos Ãštiles

```bash
# Ver estado de contenedores
docker-compose ps

# Ejecutar comandos en Airflow
docker exec -it airflow bash

# Limpiar volÃºmenes
docker-compose down -v

# Reconstruir imÃ¡genes
docker-compose build --no-cache
```

## ğŸ“ˆ Monitoreo

### MÃ©tricas del Pipeline

- **DuraciÃ³n**: Tiempo total de ejecuciÃ³n
- **Filas procesadas**: Antes/despuÃ©s de cada etapa
- **Valores faltantes**: Cantidad manejada
- **Outliers**: NÃºmero filtrado
- **Duplicados**: Cantidad removida

### Alertas

Configurar notificaciones en `default_args`:
```python
default_args = {
    'email_on_failure': True,
    'email_on_retry': False,
    'email': ['admin@empresa.com'],
}
```

## ğŸ¤ ContribuciÃ³n

1. Fork el proyecto
2. Crear branch de feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -am 'Agregar nueva funcionalidad'`)
4. Push al branch (`git push origin feature/nueva-funcionalidad`)
5. Crear Pull Request

**Nota**: AsegÃºrate de tener los datos necesarios en la ruta especificada antes de ejecutar el pipeline. Para preguntas o problemas, crear un issue en el repositorio.
